# Generated by LakehousePlumber
# Pipeline: gold_load
# FlowGroup: customer_lifetime_value
# Generated: 2025-07-14T15:38:47.847177

from pyspark.sql import DataFrame
import dlt

# Pipeline Configuration
PIPELINE_ID = "customer_lifetime_value"
PIPELINE_GROUP = "gold_load"

# ============================================================================
# SOURCE VIEWS
# ============================================================================


@dlt.view()
def v_customer_lifetime_value_sql():
    """SQL source: customer_lifetime_value_sql"""
    df = spark.sql(
        """SELECT
  c.customer_id,
  c.name as customer_name,
  c.market_segment,
  n.name as nation,
  COUNT(DISTINCT o.order_id) as total_orders,
  SUM(o.total_price) as lifetime_value,
  AVG(o.total_price) as avg_order_value,
  MIN(o.order_date) as first_order_date,
  MAX(o.order_date) as last_order_date,
  DATEDIFF(MAX(o.order_date), MIN(o.order_date)) as customer_tenure_days
FROM acmi_edw_dev.edw_silver.customer_dim c
JOIN acmi_edw_dev.edw_silver.orders_fct o ON c.customer_id = o.customer_id
JOIN acmi_edw_dev.edw_silver.nation_dim n ON c.nation_id = n.nation_id
GROUP BY c.customer_id, c.name, c.market_segment, n.name
"""
    )

    return df


# ============================================================================
# TARGET TABLES
# ============================================================================


@dlt.table(
    name="acmi_edw_dev.edw_gold.customer_lifetime_value_mv",
    comment="Materialized view: customer_lifetime_value_mv",
    table_properties={},
)
def customer_lifetime_value_mv():
    """Write to acmi_edw_dev.edw_gold.customer_lifetime_value_mv from multiple sources"""
    # Materialized views use batch processing
    df = spark.read.table("v_customer_lifetime_value_sql")

    return df
